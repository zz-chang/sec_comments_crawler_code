{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corrected data\n",
    "comment_info_all_corrected = pd.read_csv('../comment_info_all_corrected.csv')\n",
    "# create url_suffix\n",
    "comment_info_all_corrected['url_suffix'] = comment_info_all_corrected['pdf_url'].str.split('/').str[-1]\n",
    "comment_info_all_corrected['url_suffix'] = comment_info_all_corrected['url_suffix'].str.split('.').str[0]\n",
    "# adjust date\n",
    "comment_info_all_corrected.loc[8525, 'date'] = 'October 13, 2020'\n",
    "comment_info_all_corrected['date_transformed'] = pd.to_datetime(comment_info_all_corrected['date'], errors='coerce')\n",
    "\n",
    "# load length data\n",
    "single_letter_file_length_list = pd.read_csv('../single_letter_file_length_list.csv')\n",
    "single_letter_file_length_list['url_suffix'] = single_letter_file_length_list['single_letter_file_list'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "comment_info_all_corrected_merged = pd.merge(comment_info_all_corrected, single_letter_file_length_list, on='url_suffix', how='right')\n",
    "# adjust col order\n",
    "comment_info_all_corrected_merged = comment_info_all_corrected_merged[['file_no', 'com_url', 'num_tds', 'date', 'date_transformed', \n",
    "                                                                        'person', 'pdf_url', 'url_suffix', 'single_letter_file_list',\n",
    "                                                                        'content_length']]\n",
    "# drop duplicates\n",
    "comment_info_all_corrected_merged.drop_duplicates(inplace=True)\n",
    "# judge momo\n",
    "comment_info_all_corrected_merged['is_memo'] = comment_info_all_corrected_merged['person'].apply(lambda x: 1 if 'Memorandum' in str(x) else 0)\n",
    "# save\n",
    "comment_info_all_corrected_merged.drop(columns=['url_suffix','num_tds']).to_csv('../Corrected_single_letter_file_length_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged[comment_info_all_corrected_merged['url_suffix']=='s74410-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged.loc[62194, 'com_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged[comment_info_all_corrected_merged['url_suffix']=='s74510-901']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged.loc[63964, 'com_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_info_all_corrected_merged.loc[1, 'com_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小于15个字的文件筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load single letter file length list\n",
    "single_letter_file_length_list = pd.read_csv('../single_letter_file_length_list.csv')\n",
    "typical_letter_file_length_list = pd.read_csv('../typical_letter_file_length_list.csv')\n",
    "single_letter_file_length_list_less15 = single_letter_file_length_list[single_letter_file_length_list['content_length'] < 15]\n",
    "\n",
    "# copy single letter file to new folder\n",
    "for index, row in single_letter_file_length_list_less15.iterrows():\n",
    "    file_name_pdf = row['single_letter_file_list'].replace('.txt', '.pdf')\n",
    "    file_name_htm = row['single_letter_file_list'].replace('.txt', '.htm')\n",
    "    # print('../sec_single_letter_file/' + file_name_pdf)\n",
    "    if os.path.exists('../sec_single_letter_file/' + file_name_pdf):\n",
    "        original_file_path = '../sec_single_letter_file/' + file_name_pdf\n",
    "        new_file_path = '../sec_single_letter_file_length_less15/' + file_name_pdf\n",
    "        shutil.copyfile(original_file_path, new_file_path)\n",
    "    elif os.path.exists('../sec_single_letter_file/' + file_name_htm):\n",
    "        original_file_path = '../sec_single_letter_file/' + file_name_htm\n",
    "        new_file_path = '../sec_single_letter_file_length_less15/' + file_name_htm\n",
    "        shutil.copyfile(original_file_path, new_file_path)\n",
    "    else:\n",
    "        print('file not found: ' + file_name_pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
